{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam message classification\n",
    "\n",
    "As part of this assignment , we are analysing the spam messages for classifcation\n",
    "\n",
    "**Data source**\n",
    "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
    "\n",
    "Link to the recording:\n",
    "\n",
    "https://www.youtube.com/watch?v=tkuucWzI_Ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import urllib, re, nltk, string\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**\n",
    "\n",
    "The Data set contains 5574 text messages which are classified as either spam or harm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file line by into list\n",
    "#with open('D:\\Personal\\Masters\\Rathish-Personal\\CUNY\\Summer-22\\Rathish\\Week5Assignment\\SMSSpamCollection') as f:\n",
    "#    lines = [line.rstrip() for line in f]\n",
    "  \n",
    "lines =[]\n",
    "for line in urllib.request.urlopen('https://raw.githubusercontent.com/rathish-ps/Data620/main/SMSSpamCollection'):\n",
    "    lines.append(line.decode('utf-8').rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing\n",
    "Process the list if messages line by line and covert it into a list of tuples (message , label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'ham'), ('Ok lar... Joking wif u oni...', 'ham'), (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'spam')]\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])\n",
    "all_messages =[(re.split(r'\\t+', message)[1],re.split(r'\\t+', message)[0]) for message in lines]\n",
    "print(all_messages[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "We are now going to seperate the spam and ham messages to identify the top words from each category \n",
    "after removing the stop words from it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to remove the stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gt', 318), ('lt', 316), ('get', 301), ('go', 249), ('ur', 247), ('ok', 235), ('know', 231), ('like', 229), ('got', 228), ('call', 227)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ham_messages =''\n",
    "spam_messages =''\n",
    "for (msg, cls) in all_messages:\n",
    "    if cls =='ham':\n",
    "      ham_messages =   ham_messages +msg\n",
    "    if cls =='spam':\n",
    "      spam_messages =   spam_messages +msg\n",
    "    \n",
    "#print(ham_messages)    \n",
    "\n",
    "#identify the top 200 words from ham and spam messages\n",
    "pattern = r'\\w{2,}' #we want to remove unwanted characters like *, # etc\n",
    "ham_tokens = nltk.regexp_tokenize(ham_messages, pattern)\n",
    "ham_text = nltk.Text(ham_tokens)\n",
    "ham_text_lower = [word.lower() for word in ham_text if word not in stop_words]\n",
    "ham_wordFreq = nltk.FreqDist(ham_text_lower)\n",
    "ham_top200 = ham_wordFreq.most_common(200)\n",
    "print(ham_top200[:10])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('call', 344), ('free', 196), ('txt', 160), ('ur', 134), ('mobile', 126), ('text', 120), ('claim', 112), ('stop', 112), ('reply', 101), ('www', 98)]\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\w{2,}' #we want to remove unwanted characters like *, # etc\n",
    "spam_tokens = nltk.regexp_tokenize(spam_messages, pattern)\n",
    "spam_text = nltk.Text(spam_tokens)\n",
    "spam_text_lower = [word.lower() for word in spam_text if word not in stop_words]\n",
    "spam_wordFreq = nltk.FreqDist(spam_text_lower)\n",
    "spam_top200 = spam_wordFreq.most_common(200)\n",
    "print(spam_top200[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the common words from top 200 spam and ham words if any.\n",
    "def remove_common(a, b):\n",
    "  \n",
    "    for i in a[:]:\n",
    "        if i in b:\n",
    "            a.remove(i)\n",
    "            b.remove(i)\n",
    "            \n",
    "remove_common(ham_top200,spam_top200)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are preparting the feature set based on the top most of words from each category\n",
    "word_features = [x for x,y in ham_top200] + [x for x,y in spam_top200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After preprocessing, we need to extract feature from text message. So it is necessary to tokenize message and verify if it is present in the featureset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call find_features function for each SMS message\n",
    "feature_set = [(find_features(text), label) for (text, label) in all_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "We are now going to split the data into training and test set and build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set, test_set = train_test_split(feature_set, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4180\n",
      "1394\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9677187948350072"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DecisionTreeClassifier \n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classify((test_set[0])[0]))\n",
    "print(classifier.classify((test_set[0])[0]) ==(test_set[0])[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9741750358680057\n"
     ]
    }
   ],
   "source": [
    "nbClsfr = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(nbClsfr, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 service = True             spam : ham    =    142.7 : 1.0\n",
      "                      16 = True             spam : ham    =     87.3 : 1.0\n",
      "                   award = True             spam : ham    =     57.5 : 1.0\n",
      "                    draw = True             spam : ham    =     55.7 : 1.0\n",
      "                selected = True             spam : ham    =     52.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nbClsfr.show_most_informative_features(5)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
